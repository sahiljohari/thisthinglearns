{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Reinforcement Learning on Snake\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: [Sahil Johari](http://www.sahiljohari.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Need to write this_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO-DO:\n",
    "* Configurable fps for output\n",
    "* Code commenting and clean-up\n",
    "* Display score for each game -- **Done**\n",
    "* Add graphs to visualize performance of model\n",
    "* Results evaluation\n",
    "* Parallelize model for GPU\n",
    "* Add stopping condition while testing -- **Done**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import itertools as it\n",
    "import os\n",
    "from random import sample as rsample\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.layers.core import Dense, Flatten\n",
    "from keras.optimizers import SGD, RMSprop, Adam\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import display, clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Snake(object):\n",
    "    def __init__(self, rewards, grid_size):\n",
    "        self.grid_size = grid_size\n",
    "        self.snake_length = 3\n",
    "        self.Fruit = namedtuple('Fruit', ['x', 'y'])\n",
    "        self.life_reward = rewards[0]\n",
    "        self.alive_reward = rewards[1]\n",
    "        self.death_reward = rewards[2]\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.actions = [(-1, 0)] * self.snake_length  # An action for each snake segment\n",
    "        self.head_x = self.grid_size // 2 - self.snake_length // 2\n",
    "        self.snake = [(x, self.grid_size // 2) for x in range(self.head_x, self.head_x + self.snake_length)]\n",
    "        self.grow = -1  # Don't start growing snake yet\n",
    "        self.fruit = self.Fruit(-1, -1)\n",
    "        \n",
    "    def play(self):\n",
    "        self.reset()\n",
    "        while True:\n",
    "            # Draw borders\n",
    "            screen = np.zeros((self.grid_size, self.grid_size))\n",
    "            screen[[0, -1]] = 1\n",
    "            screen[:, [0, -1]] = 1\n",
    "            sum_of_borders = screen.sum()\n",
    "\n",
    "            # Draw snake\n",
    "            for segm in self.snake:\n",
    "                x, y = segm\n",
    "                screen[y, x] = 1\n",
    "\n",
    "            # Snake hit into wall or ate itself\n",
    "            end_of_game = len(self.snake) > len(set(self.snake)) or screen.sum() < sum_of_borders + len(self.snake)\n",
    "            reward = self.death_reward * end_of_game if end_of_game else self.alive_reward\n",
    "\n",
    "            # Draw fruit\n",
    "            if screen[self.fruit.y, self.fruit.x] > .5:\n",
    "                self.grow += 1\n",
    "                reward = len(self.snake) * self.life_reward\n",
    "                while True:\n",
    "                    self.fruit = self.Fruit(*np.random.randint(1, self.grid_size - 1, 2))\n",
    "                    if screen[self.fruit.y, self.fruit.x] < 1:\n",
    "                        break\n",
    "\n",
    "            screen[self.fruit.y, self.fruit.x] = .5\n",
    "\n",
    "            action = yield screen, reward, len(self.snake)-self.snake_length\n",
    "\n",
    "            step_size = sum([abs(act) for act in action])\n",
    "            if not step_size:\n",
    "                action = self.actions[0]  # Repeat last action\n",
    "            elif step_size > 1:\n",
    "                raise ValueError('Cannot move more than 1 unit at a time')\n",
    "\n",
    "            self.actions.insert(0, action)\n",
    "            self.actions.pop()\n",
    "\n",
    "            # For as long as the snake needs to grow,\n",
    "            # copy last segment, and add (0, 0) action\n",
    "            if self.grow > 0:\n",
    "                self.snake.append(self.snake[-1])\n",
    "                self.actions.append((0, 0))\n",
    "                self.grow -= 1\n",
    "\n",
    "            # Update snake segments\n",
    "            for ix, act in enumerate(self.actions):\n",
    "                x, y = self.snake[ix]\n",
    "                delta_x, delta_y = act\n",
    "                self.snake[ix] = x + delta_x, y + delta_y\n",
    "\n",
    "            if end_of_game:\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling an Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    def __init__(self, \n",
    "                 all_possible_actions,\n",
    "                 gamma=0.9, \n",
    "                 nb_epochs=1000,\n",
    "                 batch_size=32,\n",
    "                 epsilon=1,\n",
    "                 nb_frames = 4,\n",
    "                 grid_size=10,\n",
    "                 rewards=[5, -1, -10],\n",
    "                load_path=''):\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.nb_epochs = nb_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.epsilon = epsilon\n",
    "        self.action_set = all_possible_actions\n",
    "        self.nb_actions = len(self.action_set)\n",
    "        self.rewards = rewards\n",
    "        self.nb_frames = nb_frames\n",
    "        \n",
    "        self.grid_size = grid_size\n",
    "\n",
    "        self.model = self.build_model(load_path)\n",
    "        \n",
    "        self.env = Snake(self.rewards, self.grid_size)\n",
    "        \n",
    "    def build_model(self, load_path):\n",
    "        num_filters = [8]\n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(BatchNormalization(axis=1, input_shape=(self.nb_frames, self.grid_size, self.grid_size)))\n",
    "        for filters in num_filters:\n",
    "            model.add(Conv2D(filters=filters, \n",
    "                             input_shape = (self.nb_frames, self.grid_size, self.grid_size), \n",
    "                             kernel_size=(3,3), \n",
    "                             padding='same', \n",
    "                             activation='relu'))\n",
    "            model.add(MaxPooling2D(pool_size=(2, 2), strides=2, padding='same'))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(64, activation='relu'))\n",
    "        model.add(Dense(self.nb_actions, activation='linear'))\n",
    "        if load_path!='':\n",
    "            model.load_weights(load_path)\n",
    "        model.compile(optimizer=SGD(lr=0.001), loss='mse', metrics=['accuracy'])\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def model_summary(self):\n",
    "        print(self.model.summary())\n",
    "    \n",
    "    def experience_replay(self, batch_size):\n",
    "        \"\"\"\n",
    "        Coroutine of experience replay.\n",
    "\n",
    "        Provide a new experience by calling send, which in turn yields \n",
    "        a random batch of previous replay experiences.\n",
    "        \"\"\"\n",
    "        memory = []\n",
    "        while True:\n",
    "            experience = yield rsample(memory, batch_size) if batch_size <= len(memory) else None\n",
    "            memory.append(experience)\n",
    "            \n",
    "    def train(self):\n",
    "        exp_replay = self.experience_replay(self.batch_size)\n",
    "        # Start experience replay coroutine\n",
    "        next(exp_replay)\n",
    "        \n",
    "        for i in range(self.nb_epochs):\n",
    "            g = self.env.play()\n",
    "            screen, _, _ = next(g)\n",
    "            S = np.asarray([screen] * self.nb_frames)\n",
    "            try:\n",
    "                # Decrease epsilon over the first half of training\n",
    "                if self.epsilon > .2:\n",
    "                    self.epsilon -= .9 / (self.nb_epochs / 2)\n",
    "\n",
    "                loss = 0.\n",
    "                while True:\n",
    "                    if np.random.random() < self.epsilon:\n",
    "                        ix = np.random.randint(self.nb_actions)\n",
    "#                     if np.random.random() > self.epsilon:\n",
    "                    else:\n",
    "                        ix = np.argmax(self.model.predict(S[np.newaxis]), axis=-1)[0]\n",
    "\n",
    "                    action = self.action_set[ix]\n",
    "                    screen, reward, _ = g.send(action)\n",
    "                    S_prime = np.zeros_like(S) \n",
    "                    S_prime[1:] = S[:-1]\n",
    "                    S_prime[0] = screen\n",
    "                    experience = (S, action, reward, S_prime)\n",
    "                    S = S_prime\n",
    "                    \n",
    "                    # Debug\n",
    "#                     display(print(experience))\n",
    "#                     clear_output(wait=True)\n",
    "#                     time.sleep(10)\n",
    "                    \n",
    "                    batch = exp_replay.send(experience)\n",
    "                    if batch:\n",
    "                        inputs = []\n",
    "                        targets = []\n",
    "                        for s, a, r, s_prime in batch:\n",
    "                            # The targets of unchosen actions are set to the q-values of the model,\n",
    "                            # so that the corresponding errors are 0. The targets of chosen actions\n",
    "                            # are set to either the rewards, in case a terminal state has been reached, \n",
    "                            # or future discounted q-values, in case episodes are still running.\n",
    "                            t = self.model.predict(s[np.newaxis]).flatten()\n",
    "                            ix = self.action_set.index(a)\n",
    "                            if r < 0:\n",
    "                                t[ix] = r\n",
    "                            else:\n",
    "                                t[ix] = r + self.gamma * self.model.predict(s_prime[np.newaxis]).max(axis=-1)\n",
    "                            targets.append(t)\n",
    "                            inputs.append(s)\n",
    "\n",
    "                        self.model.fit(np.array(inputs), np.array(targets), epochs=1, verbose=0)\n",
    "\n",
    "            except StopIteration:\n",
    "               pass\n",
    "\n",
    "            if (i + 1) % 100 == 0 or i == 0:\n",
    "                display(print ('Epoch %6i/%i, epsilon: %.3f' % (i + 1, self.nb_epochs, self.epsilon)))\n",
    "                clear_output(wait=True)\n",
    "                \n",
    "                self.model.save_weights('snake_game_weights.h5', overwrite=True)       \n",
    "        \n",
    "        print('Training complete..')\n",
    "    \n",
    "    def render(self):\n",
    "        if 'images_new' not in os.listdir('.'):\n",
    "            os.mkdir('images_new')\n",
    "        frame_cnt = it.count()\n",
    "        while True:\n",
    "            screen = (yield)\n",
    "#             clear_output(wait=True)\n",
    "            plt.imshow(screen, interpolation='none', cmap='gray')\n",
    "#             display(plt.gcf())\n",
    "            plt.savefig('images_new/%04i.png' % (next(frame_cnt), ))\n",
    "    \n",
    "    def test(self, nb_episodes=10):\n",
    "        img_saver = self.render()\n",
    "        next(img_saver)\n",
    "        max_episode_length = 150\n",
    "        game_cnt = it.count(1)\n",
    "        \n",
    "#         self.scores = []\n",
    "#         self.rewards = []\n",
    "        \n",
    "        for _ in range(nb_episodes):\n",
    "            tot_rewards = 0\n",
    "            g = self.env.play()\n",
    "            screen, _, init_score = next(g)\n",
    "            img_saver.send(screen)\n",
    "            frame_cnt = it.count()\n",
    "            try:\n",
    "                S = np.asarray([screen] * self.nb_frames)\n",
    "                while True:\n",
    "                    next(frame_cnt)\n",
    "                    ix = np.argmax(self.model.predict(S[np.newaxis]), axis=-1)[0]\n",
    "                    screen, r, score = g.send(self.action_set[ix])\n",
    "                    S[1:] = S[:-1]\n",
    "                    S[0] = screen\n",
    "                    img_saver.send(screen)\n",
    "                    tot_rewards += r\n",
    "                    if next(frame_cnt) > max_episode_length and score-init_score == 0:\n",
    "                        raise StopIteration\n",
    "                    else:\n",
    "                        init_score = score\n",
    "            \n",
    "            except StopIteration:\n",
    "#                 self.scores.append(score)\n",
    "#                 self.rewards.append(r)\n",
    "#                 display(plt.title('Played %3i frames for game %3i with score: %d' % (next(frame_cnt), next(game_cnt), score)))\n",
    "                print('Played %3i frames for game %3i with score: %d and reward: %d' % (next(frame_cnt), next(game_cnt), score, tot_rewards))\n",
    "        \n",
    "        img_saver.close()\n",
    "#         print(self.scores, self.rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization_10 (Batc (None, 4, 10, 10)         16        \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 4, 10, 8)          728       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 2, 5, 8)           0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 80)                0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 64)                5184      \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 6,253\n",
      "Trainable params: 6,245\n",
      "Non-trainable params: 8\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "args = [\n",
    "    ((0, 0), (-1, 0), (1, 0), (0, -1), (0, 1)), #action_set\n",
    "    0.8, #gamma\n",
    "    20000, #epochs\n",
    "    64, #batch_size\n",
    "    1., #epsilon\n",
    "    4, #number of frames\n",
    "    10, #grid size\n",
    "    [5, -1, -10], #reward set\n",
    "    'snake_game_weights.h5' #load weights path\n",
    "#     ''\n",
    "    ]\n",
    "\n",
    "agent = Agent(*args)\n",
    "agent.model_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete..\n"
     ]
    }
   ],
   "source": [
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Played 152 frames for game   1 with score: 1 and reward: -60\n",
      "Played  97 frames for game   2 with score: 1 and reward: -101\n",
      "Played  71 frames for game   3 with score: 0 and reward: -145\n",
      "Played  51 frames for game   4 with score: 1 and reward: -163\n",
      "Played  65 frames for game   5 with score: 2 and reward: -167\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Agent' object has no attribute 'scores'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-7daa4e1b7b69>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnb_episodes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-26-6a4ed502c3b1>\u001b[0m in \u001b[0;36mtest\u001b[1;34m(self, nb_episodes)\u001b[0m\n\u001b[0;32m    176\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m         \u001b[0mimg_saver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 178\u001b[1;33m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'Agent' object has no attribute 'scores'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "agent.test(nb_episodes=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possible problems:\n",
    "* Model (Training parameters)\n",
    "* Rewards\n",
    "* Training iterations\n",
    "* Randomness (epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "* [Basic Reinforcement Learning by VÃ­ctor Mayoral Vilches](https://github.com/vmayoral/basic_reinforcement_learning/blob/master/tutorial6/examples/Snake/snake.py)\n",
    "\n",
    "* [Interactive Python Notebook for RL in Catch](https://gist.github.com/cadurosar/bd54c723c1d6335a43c8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
