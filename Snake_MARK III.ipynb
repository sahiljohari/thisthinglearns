{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Reinforcement Learning on Snake\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: [Sahil Johari](http://www.sahiljohari.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Need to write this_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO-DO:\n",
    "* Configurable fps for output\n",
    "* Code commenting and clean-up\n",
    "* Add graphs to visualize performance of model: Epsilon\n",
    "* Results evaluation\n",
    "* Parallelize model for GPU\n",
    "* Add stopping condition while testing -- **Need a good one**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiments:**\n",
    "* [1] Try MaxPooling only once and play with convolutional layers\n",
    "* [2] Replace ConvNet by a simple neural network\n",
    "* [3] Increase grid size and train for ~50,000 training iterations - **NOT WORKING**\n",
    "* [4] Modify the epsilon reducing rate and visualize it - **NOT WORKING**\n",
    "* [5] Change rewards - **NOT WORKING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import itertools as it\n",
    "import os\n",
    "from random import sample as rsample\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.layers.core import Dense, Flatten, Dropout\n",
    "from keras.optimizers import SGD, RMSprop, Adam\n",
    "from keras.regularizers import l2\n",
    "\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import display, clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Snake(object):\n",
    "    def __init__(self, rewards, grid_size):\n",
    "        self.grid_size = grid_size\n",
    "        self.snake_length = 3\n",
    "        self.Fruit = namedtuple('Fruit', ['x', 'y'])\n",
    "        self.life_reward = rewards[0]\n",
    "        self.alive_reward = rewards[1]\n",
    "        self.death_reward = rewards[2]\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.actions = [(-1, 0)] * self.snake_length  # An action for each snake segment\n",
    "        self.head_x = self.grid_size // 2 - self.snake_length // 2\n",
    "        self.snake = [(x, self.grid_size // 2) for x in range(self.head_x, self.head_x + self.snake_length)]\n",
    "        self.grow = -1  # Don't start growing snake yet\n",
    "        self.fruit = self.Fruit(-1, -1)\n",
    "        \n",
    "    def play(self):\n",
    "        self.reset()\n",
    "        while True:\n",
    "            # Draw borders\n",
    "            screen = np.zeros((self.grid_size, self.grid_size))\n",
    "            screen[[0, -1]] = 1\n",
    "            screen[:, [0, -1]] = 1\n",
    "            sum_of_borders = screen.sum()\n",
    "\n",
    "            # Draw snake\n",
    "            for segm in self.snake:\n",
    "                x, y = segm\n",
    "                screen[y, x] = 1\n",
    "\n",
    "            # Snake hit into wall or ate itself\n",
    "            end_of_game = len(self.snake) > len(set(self.snake)) or screen.sum() < sum_of_borders + len(self.snake)\n",
    "            reward = self.death_reward * end_of_game if end_of_game else self.alive_reward\n",
    "\n",
    "            # Draw fruit\n",
    "            if screen[self.fruit.y, self.fruit.x] > .5:\n",
    "                self.grow += 1\n",
    "                reward = len(self.snake) * self.life_reward\n",
    "                while True:\n",
    "                    self.fruit = self.Fruit(*np.random.randint(1, self.grid_size - 1, 2))\n",
    "                    if screen[self.fruit.y, self.fruit.x] < 1:\n",
    "                        break\n",
    "\n",
    "            screen[self.fruit.y, self.fruit.x] = .5\n",
    "\n",
    "            action = yield screen, reward, len(self.snake)-self.snake_length\n",
    "\n",
    "            step_size = sum([abs(act) for act in action])\n",
    "            if not step_size:\n",
    "                action = self.actions[0]  # Repeat last action\n",
    "            elif step_size > 1:\n",
    "                raise ValueError('Cannot move more than 1 unit at a time')\n",
    "\n",
    "            self.actions.insert(0, action)\n",
    "            self.actions.pop()\n",
    "\n",
    "            # For as long as the snake needs to grow,\n",
    "            # copy last segment, and add (0, 0) action\n",
    "            if self.grow > 0:\n",
    "                self.snake.append(self.snake[-1])\n",
    "                self.actions.append((0, 0))\n",
    "                self.grow -= 1\n",
    "\n",
    "            # Update snake segments\n",
    "            for ix, act in enumerate(self.actions):\n",
    "                x, y = self.snake[ix]\n",
    "                delta_x, delta_y = act\n",
    "                self.snake[ix] = x + delta_x, y + delta_y\n",
    "\n",
    "            if end_of_game:\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling an Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    def __init__(self, \n",
    "                 all_possible_actions,\n",
    "                 gamma=0.9, \n",
    "                 nb_epochs=1000,\n",
    "                 batch_size=32,\n",
    "                 epsilon=1,\n",
    "                 nb_frames = 4,\n",
    "                 grid_size=10,\n",
    "                 rewards=[5, -1, -10],\n",
    "                load_path=''):\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.nb_epochs = nb_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.epsilon = epsilon\n",
    "        self.min_epsilon = 0.1\n",
    "        self.epsilon_rate = 0.99\n",
    "        self.action_set = all_possible_actions\n",
    "        self.nb_actions = len(self.action_set)\n",
    "        self.rewards = rewards\n",
    "        self.nb_frames = nb_frames\n",
    "        \n",
    "        self.grid_size = grid_size\n",
    "\n",
    "        self.model = self.build_model(load_path)\n",
    "        \n",
    "        self.env = Snake(self.rewards, self.grid_size)\n",
    "        \n",
    "    def build_model(self, load_path):\n",
    "        num_filters = [32, 64]\n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(BatchNormalization(axis=1, input_shape=(self.nb_frames, self.grid_size, self.grid_size)))\n",
    "        for filters in num_filters:\n",
    "            model.add(Conv2D(filters=filters, \n",
    "                             input_shape = (self.nb_frames, self.grid_size, self.grid_size), \n",
    "                             kernel_size=(3,3), \n",
    "                             padding='same', \n",
    "                             activation='relu'))\n",
    "            model.add(MaxPooling2D(pool_size=(2, 2), strides=2, padding='same'))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(512, activation='relu'))\n",
    "        model.add(Dense(self.nb_actions, activation='linear'))\n",
    "                       \n",
    "        if load_path!='':\n",
    "            model.load_weights(load_path)\n",
    "        model.compile(optimizer=RMSprop(lr=0.01), loss='mse', metrics=['accuracy'])\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def model_summary(self):\n",
    "        print(self.model.summary())\n",
    "    \n",
    "    def experience_replay(self, batch_size):\n",
    "        \"\"\"\n",
    "        Coroutine of experience replay.\n",
    "\n",
    "        Provide a new experience by calling send, which in turn yields \n",
    "        a random batch of previous replay experiences.\n",
    "        \"\"\"\n",
    "        memory = []\n",
    "        while True:\n",
    "            experience = yield rsample(memory, batch_size) if batch_size <= len(memory) else None\n",
    "            memory.append(experience)\n",
    "            \n",
    "    def train(self):\n",
    "        exp_replay = self.experience_replay(self.batch_size)\n",
    "        # Start experience replay coroutine\n",
    "        next(exp_replay)\n",
    "        \n",
    "        epsilon_set = []\n",
    "        \n",
    "        for i in range(self.nb_epochs):\n",
    "            g = self.env.play()\n",
    "            screen, _, _ = next(g)\n",
    "            S = np.asarray([screen] * self.nb_frames)\n",
    "            try:\n",
    "                # Decrease epsilon over the first half of training\n",
    "                if self.epsilon > self.min_epsilon:\n",
    "                    self.epsilon -= self.epsilon_rate / (self.nb_epochs / 2)\n",
    "#                     self.epsilon *= self.epsilon_rate\n",
    "                    epsilon_set.append(self.epsilon)\n",
    "                while True:\n",
    "                    if np.random.random() < self.epsilon:\n",
    "                        ix = np.random.randint(self.nb_actions)\n",
    "                    else:\n",
    "                        ix = np.argmax(self.model.predict(S[np.newaxis]), axis=-1)[0]\n",
    "\n",
    "                    action = self.action_set[ix]\n",
    "                    screen, reward, _ = g.send(action)\n",
    "                    S_prime = np.zeros_like(S) \n",
    "                    S_prime[1:] = S[:-1]\n",
    "                    S_prime[0] = screen\n",
    "                    experience = (S, action, reward, S_prime)\n",
    "                    S = S_prime\n",
    "                    \n",
    "                    # Debug\n",
    "#                     display(print(experience))\n",
    "#                     clear_output(wait=True)\n",
    "#                     time.sleep(10)\n",
    "                    \n",
    "                    batch = exp_replay.send(experience)\n",
    "                    if batch:\n",
    "                        inputs = []\n",
    "                        targets = []\n",
    "                        for s, a, r, s_prime in batch:\n",
    "                            # The targets of unchosen actions are set to the q-values of the model,\n",
    "                            # so that the corresponding errors are 0. The targets of chosen actions\n",
    "                            # are set to either the rewards, in case a terminal state has been reached, \n",
    "                            # or future discounted q-values, in case episodes are still running.\n",
    "                            t = self.model.predict(s[np.newaxis]).flatten()\n",
    "                            ix = self.action_set.index(a)\n",
    "                            if r < 0:\n",
    "                                t[ix] = r\n",
    "                            else:\n",
    "                                t[ix] = r + self.gamma * self.model.predict(s_prime[np.newaxis]).max(axis=-1)\n",
    "                            targets.append(t)\n",
    "                            inputs.append(s)\n",
    "\n",
    "                        self.model.fit(np.array(inputs), np.array(targets), epochs=1, verbose=0)                \n",
    "            except StopIteration:\n",
    "               pass\n",
    "\n",
    "#             if (i + 1) % 500 == 0 or i == 0:\n",
    "            display(print ('Epoch %6i/%i, epsilon: %.4f' % (i + 1, self.nb_epochs, self.epsilon)))\n",
    "            clear_output(wait=True)\n",
    "        \n",
    "            if (i + 1) % 500 == 0 or i == 0:\n",
    "                self.model.save_weights('snake_game_weights_final.h5', overwrite=True)       \n",
    "        \n",
    "        print('Training complete..\\n')\n",
    "#         print('Exploration rate: %1f' %(eps_exploration/eps_total))\n",
    "#         print('Exploitation rate: %1f' %(1-(eps_exploration/eps_total)))\n",
    "#         plt.plot(epsilon_set, label=\"Epsilon\")\n",
    "    \n",
    "    def render(self):\n",
    "        if 'images' not in os.listdir('.'):\n",
    "            os.mkdir('images')\n",
    "        frame_cnt = it.count()\n",
    "        while True:\n",
    "            screen = (yield)\n",
    "            clear_output(wait=True)\n",
    "            plt.imshow(screen, interpolation='none', cmap='gray')\n",
    "            display(plt.show())\n",
    "#             plt.savefig('images/%04i.png' % (next(frame_cnt), ))\n",
    "    \n",
    "    def test(self, nb_episodes=10):\n",
    "        img_saver = self.render()\n",
    "        next(img_saver)\n",
    "        game_cnt = it.count(1)\n",
    "        \n",
    "        self.scores = []\n",
    "        self.rewards = []\n",
    "        \n",
    "        for _ in range(nb_episodes):\n",
    "            tot_reward = 0\n",
    "            frame_cap = 0\n",
    "            max_score = 7\n",
    "            max_episode_length = 200\n",
    "            stopping_reward = -350\n",
    "            g = self.env.play()\n",
    "            screen, _, init_score = next(g)\n",
    "            img_saver.send(screen)\n",
    "            frame_cnt = it.count()\n",
    "            try:\n",
    "                S = np.asarray([screen] * self.nb_frames)\n",
    "                while True:\n",
    "                    next(frame_cnt)\n",
    "                    ix = np.argmax(self.model.predict(S[np.newaxis]), axis=-1)[0]\n",
    "                    screen, r, score = g.send(self.action_set[ix])\n",
    "                    S[1:] = S[:-1]\n",
    "                    S[0] = screen\n",
    "                    img_saver.send(screen)\n",
    "                    \n",
    "                    tot_reward += r\n",
    "                    \n",
    "                    if r == 5:\n",
    "                        frame_cap = next(frame_cnt)\n",
    "                    \n",
    "                    if next(frame_cnt)-frame_cap > max_episode_length*(score+1):\n",
    "                        raise StopIteration\n",
    "                            \n",
    "#                     if next(frame_cnt) > max_episode_length and abs(score-init_score) in range(max_score+1):\n",
    "#                     if tot_reward < stopping_reward and score < max_score:\n",
    "#                         raise StopIteration\n",
    "#                     else:\n",
    "#                         max_score += 3\n",
    "#                         stopping_reward -= 50\n",
    "#                         init_score = score\n",
    "#                         max_episode_length += 0.25\n",
    "            \n",
    "            except StopIteration:\n",
    "                self.scores.append(score)\n",
    "                self.rewards.append(tot_reward)\n",
    "#                 plt.title('Played %3i frames for game %3i with score: %d' % (next(frame_cnt), next(game_cnt), score))\n",
    "#                 print('Played %3i frames for game %3i with score: %d and reward: %d' % (next(frame_cnt), next(game_cnt), score, tot_reward))\n",
    "#                 print('Played %3i frames for game %3i with score: %d' % (next(frame_cnt), next(game_cnt), score))\n",
    "        \n",
    "        img_saver.close()\n",
    "#         plt.bar(range(len(self.scores)),self.scores)\n",
    "        return self.scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def playSnake(train=False, \n",
    "              test=True, \n",
    "              training_iterations=5000, \n",
    "              test_episodes=10):\n",
    "    \n",
    "    params = dict(\n",
    "        all_possible_actions=((0, 0), (-1, 0), (1, 0), (0, -1), (0, 1)),\n",
    "                     gamma=0.618, \n",
    "                     nb_epochs=training_iterations,\n",
    "                     batch_size=64,\n",
    "                     epsilon=1,\n",
    "                     nb_frames = 2,\n",
    "                     grid_size=14,\n",
    "                     rewards=[15, -1, -10],\n",
    "                    load_path=''\n",
    "    )\n",
    "\n",
    "    target_score = 7\n",
    "    max_scores = []\n",
    "    attempts = 1\n",
    "    \n",
    "    agent = Agent(**params)\n",
    "    agent.model_summary()\n",
    "    \n",
    "    for attempt in range(attempts):\n",
    "        if test:\n",
    "            # ====Testing the model====\n",
    "            scores=agent.test(nb_episodes=test_episodes)\n",
    "            max_scores.append(max(scores))\n",
    "            if max(scores) == target_score:\n",
    "                print(\"\\n==========\\nTarget achieved successfully!\\n==========\")\n",
    "                print(\"Number of attempts: %d\" % attempt)\n",
    "                \n",
    "                plt.plot(max_scores, label='Max scores per attempt')\n",
    "                break\n",
    "            else:\n",
    "                plt.plot(max_scores, label='Max scores per attempt')\n",
    "                plt.bar(range(len(scores)),scores)\n",
    "        if train:\n",
    "            # ====Training the model====\n",
    "            agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1576/5000, epsilon: 0.3759\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "playSnake(train=True, test=False, test_episodes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possible problems:\n",
    "* Model (Training parameters)\n",
    "* Rewards\n",
    "* Training iterations\n",
    "* Randomness (epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Common Observations:\n",
    "* Most episodes have a fixed set of moves running in an loop\n",
    "* If the snake keeps growing well, it usually bumps into itself\n",
    "* Very good at avoiding walls, but avoids food as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "* [Basic Reinforcement Learning by VÃ­ctor Mayoral Vilches](https://github.com/vmayoral/basic_reinforcement_learning/blob/master/tutorial6/examples/Snake/snake.py)\n",
    "\n",
    "* [Interactive Python Notebook for RL in Catch](https://gist.github.com/cadurosar/bd54c723c1d6335a43c8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
